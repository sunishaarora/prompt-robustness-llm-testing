# Investigating Prompt Robustness in Code-Generating Large Language Models: An Empirical Study Across Diverse Prompt Categories

## Authors
**Sunisha Arora**  
University of St. Thomas, St. Paul, MN, USA  
sunisha.arora1309@gmail.com  

**Bicky Sharma**  
University of St. Thomas, St. Paul, MN, USA  
bickysharma063@gmail.com  

## Abstract
The increasing integration of code-generating Large Language Models (LLMs) into software development workflows necessitates a thorough understanding of their reliability under various input conditions. This paper presents an empirical study evaluating the robustness of three state-of-the-art LLMs – ChatGPT 4o, Gemini 2.0 Flash, and Claude 3.7 Sonnet – when prompted with variations across five fundamental coding tasks. We analyze the impact of four distinct prompt categories: baseline, ambiguous, negated, and contradictory, on the syntactic and functional correctness, as well as the robustness and potential security implications of the generated code. Our findings reveal significant differences in the models’ ability to handle these prompt styles, highlighting critical considerations for developers and researchers utilizing LLMs for code generation, particularly in safety-critical applications. The consolidated outputs from this study are publicly available in a GitHub repository.

## Keywords
Large Language Models, Code Generation, Prompt Robustness, Software Testing, AI Testing, Empirical Study
